# Malware Prediction Kaggle Competition


## Objective
The goal of this competition is to predict a Windows machine’s probability of getting infected by various families of malware, based on different properties of that machine.

## Dataset

This dataset is provided by Microsoft. The telemetry data containing these properties and the machine infections was generated by combining heartbeat and threat reports collected by Microsoft's endpoint protection solution, Windows Defender.

Each row in this dataset corresponds to a machine, uniquely identified by a MachineIdentifier. HasDetections is the ground truth and indicates that Malware was detected on the machine. Using the information and labels in train.csv, you must predict the value for HasDetections for each machine in test.csv.

The training dataset malware_train.csv contains more than **8 million** labeled instances. There are **83 features** of the machine as shown including the target variable.

Follow the link to download the dataset
https://www.kaggle.com/c/microsoft-malware-prediction/data

## Metric
AUC (area under the curve) is uses as metric to evaluate the performance of the model.

## Data Analysis
**Power BI** is one of the tools we used for data analysis. Some of the features are emliminated based on the following criteria:
- Percentage of values in the column that are empty. If for instance, more than 90% of the values in the column were empty, then we know that this column plays no role in what the value of the outcome variable HasDetections would be. Therefore we eliminate such a column.

- Percentage of values in the column that are NA (not available). This is the same as the previous criteria if more that 90% of the values in this column are na, we know this column does not influence HasDetections, or more intuitively, the value of this feature does not play a significant role in predicting whether the machine is infected by malware or not. We eliminate such a feature.

- How do the non-null and non-empty values of a column influence the value of the outcome variable. To do this, we would
examine what possible values this feature has, and for each of those values, how are the values of the outcome variable HasDetections distributed. If the distribution of HasDetections for all the possibly values of the feature is roughly 50%, then we do not consider such an example.

Using the above criteria, we analyzed every possible feature, and narrowed down the number of features from 83 to 23.


After we implemented the XGBoost algorithm, we used the **feature_importance** method in XGBoost to obtain the relative imporatance of features. The final list used for the model is:
- SmartScreen
- AVProductsInstalled
- AVProductStatesIdentifier
- AvSigVersion
- AppVersion
- Census_TotalPhysicalRAM
- EngineVersion
- OsBuildLab
- Wdft_IsGamer
- Census_InternalPrimaryDiagonalDisplaySizeInInches
- Wdft_RegionIdentifier
- Census_OSInstallTypeName
- Census_PrimaryDiskTotalCapacity
- CountryIdentifier
- Census_ActivationChannel


## Model: XGBoost
We used XGBoost model to do the prediciton. XGBoost stands for Extreme Gradient Boosting. XGBoost is an implementation of gradient boosted decision trees designed for speed and performance. Some of the important features of XGBoost are:
- Parallelization of tree construction using all of your CPU cores during training.
- Built-in support for filling missing values.
- Standard GBM implementation has no regularization like XGBoost, therefore it also helps to reduce overfitting.
- XGBoost allows users to define custom optimization objectives and evaluation criteria.
- XGBoost allows user to run a cross-validation at each iteration of the boosting process and thus it is easy to get the exact optimum number of boosting iterations in a single run.

### Parameter Tuning
Out of all the available features, we tuned some of them manually by changing the values in increments. We finally took the values which gave us better accuracy. When we tried different different values for all these parameters, we did not use the entire dataset as it would take more time to train. So, we took a smaller dataset which can best represent the entire dataset.
After trying different values, we finalized the following values for our model.

- booster=’gbtree’
- eta=0.1
- max_depth=9
- objective=’binary_logistic’
- eval_metric=’auc’
- min_child_weight=6
- gamma=0.0
- subsample=1

## Results
We divided the dataset into training set (75%) and validation set (25%) by using train_test_split method from scikit-learn package. The results obtained on the boosting algorithms are better than normal models because they are ensemble models which combine multiple weak learners, say decision trees, to give a strong model. The aggregate opinion of a multiple models is less noisy than individual models. The two major benefits of ensemble models are better prediction and more stable model.

The list of models and their AUCs:

| Model         | AUC           |
| ------------- | ------------- |
| Logistic Regression  | 0.54  |
| Decision Trees  | 0.57  |
| AdaBoost  | 0.62  |
| LGBM  | 0.63  |
| XGBoost  | 0.65  |

It took 101 minutes to train 75% of the dataset using XGBoost. The final results obtained after testing the model on the actual test data are slightly different from the ones reported above. The final AUC on the test data is 0.63 when XGBoost is used.
